---

# Example Goal - Demonstrate that:
#  (1) a suite can consist of multiple experiments that are executed in parallel (here 2 experiments)
#  (2) `init_roles` in host_type defines an ansible role to install packages etc. on a host
#  (3) a `config.json` file with the run config is within the working directory of the `$CMD$`
#     -> can be used to pass config parameters
#  (4) we can repeat each run configuration multiple times (`n_repetitions`).
#  (5) an `$ETL$` pipeline can automatically process result files (e.g., extract from result structure, transform into a suited df, load a summary)
#  (6) we can use the Python range syntax do define $FACTORS$ via jinja
#  (7) some variables can only be known at runtime (e.g., cloud dependent), for these we can use the `at_runtime` filter to read the variable out of `exp_host_lst`

# experiment with two factors: one with 3 levels and one with 2 levels (cross-product). We repeat each run config twice.
experiment_1:
  n_repetitions: 2  # (4) each of the 6 run configurations is repeated 2x -> 12 jobs
  host_types:
    small:
      n: 1
      init_roles: setup-small
      # runs the python script demo_latency.py within the code directory fetched from the code repository: the demo script creates a result file in csv or json
      # (3) passes arguments on the command line
      $CMD$: "[% my_run.python_run %]/demo_project/demo_latency.py --opt [% my_run.opt %] --size [% my_run.payload_size_mb %] --out [% my_run.out %]"
  base_experiment:
    # (7) Uses the `| at runtime(exp_host_lst)` to access a hostvar (info: at_runtime w/o additional arguments requires that the variable has the same value across all host types)
    python_run: "[% 'demo_project_python' | at_runtime(exp_host_lst) %] [% 'exp_code_dir' | at_runtime(exp_host_lst) %]"
    out: json
    payload_size_mb:
      $FACTOR$: [10, 20, 30]
    opt:
      $FACTOR$: [True, False]

# experiment with one factor, with 3 levels, and three repetitions (each)
experiment_2:
  n_repetitions: 3 # (4) each of the 3 run configurations is repeated 3x -> 9 jobs
  host_types:
    small:
      n: 1
      init_roles: setup-small
      $CMD$: "[% my_run.python_run %]/demo_project/demo.py --config config.json" # (3) uses the config.json in the working dir of the command with the run config

  base_experiment:
    python_run: "[% 'demo_project_python' | at_runtime(exp_host_lst) %] [% 'exp_code_dir' | at_runtime(exp_host_lst) %]"
    out: csv
    problem:
      opt: False
      # (6) Python Range syntax to define factors via jinja (also possible for non-factors)
      size:
        $FACTOR$: {{ range(10, 25, 5) | list }}  # [10, 15, 20]
        # other range syntax: {{ range(3) | list }} -> [0, 1, 2]   |   {{ range(1,3) | list }} -> [1,2]  |  {{ range(20, 40, 10) | list }} -> [20, 30]
      other: "{{ range(2) | list }}"

# (5) the suite has two etl pipelines to process results
#      - each pipeline starts with a set of extractors, each produced result file is assigned to exactly one extractor (using a `file_regex`)
#         (e.g., the JsonExtractor default regex matches all files ending with `.json`)
#      - we combine the results from different extractors into a dataframe and transform it with a chain of transformers.
#         (e.g., the RepAggTransformer aggregates over all repetitions of an experiment run and calculates `mean`, `std` etc.)
#      - finally, all loaders are executed on the dataframe resulting from the chain of transformers
$ETL$:
  pipeline1:
    experiments: [experiment_1]
    extractors:
      JsonExtractor: {} # with default file_regex
      ErrorExtractor: {} # if a non-empty file exists matching the default regex -> then we throw an error using the ErrorExtractor
      IgnoreExtractor: {} # since we want that each file is processed by an extractor, we provide the IgnoreExtractor which can be used to ignore certain files. (e.g., stdout)
    transformers:
      - name: RepAggTransformer # aggregate over all repetitions of a run and calc `mean`, `std`, etc.
        data_columns: [latency] # the names of the columns in the dataframe that contain the measurements
    loaders:
      CsvSummaryLoader: {skip_empty: True} # write the transformed detl_info["suite_dir"]ataframe across the whole experiment as a csv file
      DemoLatencyPlotLoader: {} # create a plot based on project-specific plot loader


  pipeline2:
    experiments: [experiment_2]
    extractors: # with overwritten default file_regex
      CsvExtractor:
        file_regex: '.*\.csv$'
      ErrorExtractor:
        file_regex: '^stderr.log$'
      IgnoreExtractor:
        file_regex: '^stdout.log$'
    transformers:
      # sort values by [run, rep] to ensure the comparison of the test matches
      - df.sort_values: { by: [run, rep], ignore_index: yes }
    loaders:
      CsvSummaryLoader: {skip_empty: True}