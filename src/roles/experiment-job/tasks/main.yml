---

# This role, is responsible for running jobs.
#
# First, it determines if it needs to enqueue (i.e., start) a new job.
# The result of this is a variable: `exp_jobs_to_enqueue` that contains
# a list with all required information to enqueue a new job in the task spooler.
# (if if is a single instance experiment -> enqueue all pending jobs
# else + there is no job currently running -> enqueue the first pending job )
#
# Second, for all jobs to enqueue it prepares the environment on the host
# (e.g., creates directories + writes config.json file)
#
# After giving the jobs to the task spooler, we wait for the highest prio job to finish.
# (we do this in a loop until loop with `job_n_tries` retries and a delay of `job_check_wait_time` in between)
#
# When the job finishes, then we download the results and update the state.
# -> When the role ends, either the number of tries is exceeded and the playbook ends, or then ONE job is finished
#     and we downloaded the results. Since this role is called in a loop for each job, as long as the number of tries never exceeds,
#     we finish all jobs of an experiment.

- assert:
    that:
    - inventory_hostname in groups['is_controller_yes']
    - is_controller # this role is running on a controller host (one per experiment is controller)
    - exp_name is defined
    - suite_id is defined
    - job_check_wait_time is defined
    - exp_runs_ext is defined
    - exp_host_lst is defined # list of hosts involved in experiment
        #[{"host_type": x, "exp_host_type_idx": x, "exp_host_type_n": x, "is_controller": x, "public_dns_name": x, "private_dns_name": x}]

#- debug:
#    var: exp_host_lst
#
#- debug:
#    msg=' ip= {{ exp_host_lst | json_query("[?host_type=='server'].private_dns_name") | first }} '


- name: mark that from here up to the status check, no error is expected
  set_fact:
  # in the calling playbook we have a rescue block that should handle the error when the number of retries is exceeded.
    # -> however, all other errors should be thrown and not rescued -> this info variable helps to detect where the error occurred in the role
    is_expected_error: False


###################################################################
#  Start: Schedule new Jobs (enqueue task in task spooler)        #
###################################################################

- name: Schedule new Jobs
  # only schedule new jobs when there are no jobs queued or running atm
  when: exp_job_ids_pending | length > 0 and exp_job_ids_queued | length == 0 and exp_job_ids_running | length == 0
  block:

  - set_fact:
      is_single_instance_exp: "{{ exp_host_lst | length == 1}}"

  - name: Enqueue all jobs from pending because it is a single instance experiment (no synchronization required)
    set_fact:
      exp_job_ids_to_enqueue: "{{ exp_job_ids_pending }}"
    when: is_single_instance_exp

  - name: Enqueue only one job from pending because it is a multi instance experiment (synchronization required)
    set_fact:
        exp_job_ids_to_enqueue: "{{ [exp_job_ids_pending[0]] }}"
    when: not is_single_instance_exp


  - assert:
      that:
      - exp_job_ids_queued | length == 0

  - name: build job schedule list (for each host of experiment, information about job and what is required to run)
    set_fact:
      exp_jobs_to_enqueue: "{{ exp_job_ids_to_enqueue | to_job_schedule_lst(exp_host_lst, exp_runs_ext, remote.results_dir) }}"


    ###################################################################
    #  Prepare Experiment Environment (directories, files, ...)       #
    ###################################################################

  - name: Create results and scratch directories (per-host)
    ansible.builtin.shell: |
      mkdir -m755 -p {{ '{' }}{{ jobs_grouped | last | map(attribute='exp_working_dir') | map('regex_replace', '^(.*)$', '\1/scratch') | join(',') }}{{ '}' }}
      mkdir -m755 -p {{ '{' }}{{ jobs_grouped | last | map(attribute='exp_working_dir') | map('regex_replace', '^(.*)$', '\1/results') | join(',') }}{{ '}' }}
    delegate_to: "{{ jobs_grouped | first }}" # first entry in group
    loop: "{{ exp_jobs_to_enqueue | groupby('host_info.public_dns_name') }}"
    loop_control:
        loop_var: jobs_grouped

  - name: Create run config file in working directory
    template:
        src: config.json.j2
        dest: "{{ job.exp_working_dir }}/config.json"
        mode: 0755
    delegate_to: "{{ job.host_info.public_dns_name }}"
    loop: "{{ exp_jobs_to_enqueue }}"
    loop_control:
        loop_var: job

    ###################################################################
    #  Enqueue all Jobs                                               #
    ###################################################################

  - name: Clear task spooler queue on each host (+ stop all running jobs)
    tsp:
        clear_tasks: True
    delegate_to: "{{ host }}"
    loop: "{{ groups[exp_name] }}"
    loop_control:
      loop_var: host

  - name: Enqueue all jobs on the task spooler
    tsp:
        cmd: "{{ job.exp_run_cmd }}"
        cmd_label: "{{ job.job_info | to_json | string }}"
        cmd_working_dir: "{{ job.exp_working_dir }}"
        cmd_stdout_file: results/stdout.log # relative path compared to working dir
        cmd_stderr_file: results/stderr.log
    delegate_to: "{{ job.host_info.public_dns_name }}"
    loop: "{{ exp_jobs_to_enqueue }}"
    loop_control:
        loop_var: job

  - name: Update experiment state
    set_fact:
      exp_job_ids_queued: "{{ exp_job_ids_to_enqueue }}"
      exp_job_ids_pending: "{{  exp_job_ids_pending | difference(exp_job_ids_to_enqueue) }}"
      exp_job_ids_to_enqueue: []

###################################################################
#  End: Schedule new Jobs (enqueue task in task spooler)        #
###################################################################




###################################################################
#  Wait and the Get Status of All Job                             #
###################################################################


- name: Set the job id to wait until finished + update state
  set_fact:
    job_id_to_wait_for: "{{ cur_job_id }}"
    exp_job_ids_queued: "{{ exp_job_ids_queued | difference([cur_job_id]) }}"
    exp_job_ids_running: "{{ [cur_job_id] }}"
  vars:
    # if there is still a running_job_id then take this one, else take the first of the queued jobs
    cur_job_id:  "{{ (exp_job_ids_running | list + exp_job_ids_queued | list) | first }}"


- name: Save the updated state of the experiment run (save job ids)
  include_role:
    name: experiment-state
  vars:
    expstate: save


- name: mark that in the next task, there could be an expected error (number of retires exceeded)
  set_fact:
    is_expected_error: True

- name: Get status of job
  # Note: if the number of tries are exceeded, the task raises an error which stops this role and is caught in the parent
  tsp_info:
  register: tsp_result
  until: (tsp_result.tasks | tsp_job_finished(job_id_to_wait_for)) | bool
  retries: "{{ job_n_tries }}"
  delay: "{{ job_check_wait_time }}"
  delegate_to: "{{ host }}"
  loop: "{{ groups[exp_name] | intersect(groups['check_status_yes']) }}" # only check status of jobs for host_types with 'check_status' == true
  loop_control:
    loop_var: host

- name: mark that from here on, any error is unexpected
  set_fact:
    is_expected_error: False

###################################################################
#  Download Results for newly finished job                        #
###################################################################

- name: Download Results
  block:

  - debug:
      msg:
      - "remote_results_dir={{ remote_results_dir }}"
      - "local_results_dir_base={{ local_results_dir_base }}"
      - "remote_config_file= {{ remote_config_file }}"

  - name: Create local folder (for results)
    delegate_to: localhost
    file:
      path: "{{ local_results_dir_base }}/{{ my_host.host_type }}/host_{{ my_host.exp_host_type_idx }}"
      state: directory
      mode: 0755
    loop: "{{ exp_host_lst }}"
    loop_control:
      loop_var: my_host

  - name: Fetch Results (if experiment done)
    delegate_to: localhost
    local_action: command rsync -az "{{ my_host.public_dns_name }}:{{ remote_results_dir }}/*" "{{ local_results_dir_base }}/{{ my_host.host_type }}/host_{{ my_host.exp_host_type_idx }}"
    loop: "{{ exp_host_lst }}"
    loop_control:
      loop_var: my_host

  - name: Save the config of the job
    delegate_to: localhost
    local_action: "command rsync -az '{{ inventory_hostname }}:{{ remote_config_file }}' '{{ local_results_dir_base }}'"

# TODO [nku] encapsulate etl processing failure (an error in etl stage must not stop the suite)-> if fail: catch, write error message to results_dir + print_tag stdout
  - name: Run ETL pipeline over results files
    delegate_to: localhost
    ansible.builtin.shell:
      cmd: python scripts/etl.py --suite {{ suite }} --id {{ suite_id }}

  vars:
  - remote_results_dir: "{{ job_id_to_wait_for | jobid2workingdir(remote.results_dir) + '/results' }}"
  - local_results_dir_base: "{{ job_id_to_wait_for | jobid2workingdir(local.results_dir) }}"
  - remote_config_file: "{{ job_id_to_wait_for | jobid2workingdir(remote.results_dir) + '/config.json' }}"

###################################################################
#  Cleanup task spooler queue                                     #
###################################################################

- name: Remove finished job with downloaded results from queue
  tsp:
    remove_task_label: "{{ job_id_to_wait_for | to_json | string }}"
  delegate_to: "{{ host }}"
  loop: "{{ groups[exp_name] }}"
  loop_control:
    loop_var: host

###################################################################
#  Update experiment state                                        #
###################################################################

- name:
  set_fact:
    exp_job_ids_running: []
    exp_job_ids_finished: "{{ exp_job_ids_finished | list + [job_id_to_wait_for] }}"

- name:
  set_fact:
    exp_job_ids_unfinished: "{{ exp_job_ids_pending | list + exp_job_ids_queued | list + exp_job_ids_running | list}}"

- name: Save the updated state of the experiment run (save job ids)
  include_role:
    name: experiment-state
  vars:
    expstate: save


###################################################################
#  Compute suite progress info and output                         #
###################################################################

- name: compute overview of progress in different experiments in suite
  set_fact:
    suite_progress_info: "{{ suite_progress_info | default({}) | combine({my_exp_name: {'require_suite_to_finish': my_require_suite ,'n_finished': my_n_finished | int, 'n_unfinished': my_n_unfinished | int, 'progress':  (my_n_finished | int / (my_n_finished | int + my_n_unfinished | int) * 100) | round(2) | string + ' %' }})}}"
  vars:
    my_exp_name: "{{ hostvars[my_controller_host].exp_name }}"
    my_n_unfinished: "{{ hostvars[my_controller_host].exp_job_ids_unfinished | length }}"
    my_require_suite: "{{ hostvars[my_controller_host].exp_job_ids_pending | length > 0 }}"
    my_n_finished: "{{ hostvars[my_controller_host].exp_job_ids_finished | length }}"
  loop: "{{ groups['is_controller_yes'] }}"
  loop_control:
    loop_var: my_controller_host

- name: output progress information of experiments
  debug:
    msg: "{{ suite_progress_info[item] }}"
  loop: "{{ suite_progress_info.keys() | sort }}"
  tags: [print_action]